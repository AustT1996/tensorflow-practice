{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression with tensorflow\n",
    "## Done for Hands On ML chapter 9 exercise 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAETCAYAAADzrOu5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkomIiESmZCIiIpH9f44gP8McwogAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a48dbe0c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import data\n",
    "from sklearn.datasets import make_moons\n",
    "n_samples = 100\n",
    "X_moons, y_moons = make_moons(n_samples=n_samples, noise=0.15, random_state=42)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X_moons, y_moons, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on batch sizes / percentages\n",
    "n_train_iterations = 5000\n",
    "batch_size = 25\n",
    "learn_rate = 5e-3\n",
    "\n",
    "def get_moons_batch():\n",
    "    indices = np.random.randint(n_samples, size=batch_size)\n",
    "    X_ = X_moons[indices, :]\n",
    "    y_ = y_moons[indices].reshape(-1, 1).astype(np.float32)\n",
    "    return X_, y_\n",
    "\n",
    "# Set up variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "theta = tf.Variable(initial_value=np.random.random((2, 1))*2-1, \n",
    "                    dtype=tf.float32, name=\"weights\")\n",
    "\n",
    "# Loss function\n",
    "epsilon = 1e-10\n",
    "p = 1. / (1. + tf.exp(-1 * tf.matmul(X, theta)))\n",
    "loss_func = - (tf.matmul(tf.transpose(y), tf.log(p + epsilon)) +\n",
    "    tf.matmul(tf.transpose(1. - y), tf.log(1. - p + epsilon)))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "training_op = optimizer.minimize(loss_func)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start theta:\n",
      "[[-0.30145898]\n",
      " [ 0.55814683]]\n",
      "\n",
      "Batch #0: loss = 22.089109420776367\n",
      "Batch #1000: loss = 6.039754867553711\n",
      "Batch #2000: loss = 8.226639747619629\n",
      "Batch #3000: loss = 7.881573677062988\n",
      "Batch #4000: loss = 2.2746455669403076\n",
      "\n",
      "\n",
      "Best Score: 0.88\n",
      "Best theta:\n",
      "[[ 1.40956116]\n",
      " [-4.17738676]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Print start theta\n",
    "    print(\"Start theta:\\n{}\\n\".format(theta.eval()))\n",
    "    for batch_num in range(n_train_iterations):\n",
    "        X_batch, y_batch = get_moons_batch()\n",
    "        _, p_val, loss = sess.run([training_op, p, loss_func], feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # Print loss function\n",
    "        if batch_num % (n_train_iterations // 5) == 0:\n",
    "            print(\"Batch #{}: loss = {}\".format(batch_num, loss[0, 0]))\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "    # Evaluate accuracy score\n",
    "    p_final = sess.run(p, feed_dict={X: X_moons})\n",
    "    best_score = accuracy_score(y_moons, np.round(p_final))\n",
    "\n",
    "    \n",
    "# Display the best theta\n",
    "print(\"\\n\\nBest Score: {}\".format(best_score))\n",
    "\n",
    "print(\"Best theta:\\n{}\\n\".format(best_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start theta:\n",
      "[[-0.62637001]\n",
      " [ 0.97536504]]\n",
      "\n",
      "Batch #0: loss = 1.093448281288147\n",
      "Batch #2000: loss = 0.44453829526901245\n",
      "Batch #4000: loss = 0.31966808438301086\n",
      "Batch #6000: loss = 0.19728673994541168\n",
      "Batch #8000: loss = 0.339651495218277\n",
      "\n",
      "\n",
      "Best Score: 0.86\n",
      "Best theta:\n",
      "[[ 1.29450512]\n",
      " [-2.87061477]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try again using better built-in functions\n",
    "\n",
    "# Information on batch sizes / percentages\n",
    "n_train_iterations = 10000\n",
    "batch_size = 25\n",
    "learn_rate = 5e-3\n",
    "\n",
    "def get_moons_batch():\n",
    "    indices = np.random.randint(n_samples, size=batch_size)\n",
    "    X_ = X_moons[indices, :]\n",
    "    y_ = y_moons[indices].reshape(-1, 1).astype(np.float32)\n",
    "    return X_, y_\n",
    "\n",
    "# Set up variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "theta = tf.Variable(initial_value=np.random.random((2, 1))*2-1, \n",
    "                    dtype=tf.float32, name=\"weights\")\n",
    "\n",
    "# Loss function\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "p = tf.sigmoid(logits)\n",
    "loss_func = tf.losses.log_loss(y, p)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "training_op = optimizer.minimize(loss_func)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Print start theta\n",
    "    print(\"Start theta:\\n{}\\n\".format(theta.eval()))\n",
    "    for batch_num in range(n_train_iterations):\n",
    "        X_batch, y_batch = get_moons_batch()\n",
    "        _, p_val, loss = sess.run([training_op, p, loss_func], feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # Print loss function\n",
    "        if batch_num % (n_train_iterations // 5) == 0:\n",
    "            print(\"Batch #{}: loss = {}\".format(batch_num, loss))\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "    # Evaluate accuracy score\n",
    "    p_final = sess.run(p, feed_dict={X: X_moons})\n",
    "    best_score = accuracy_score(y_moons, np.round(p_final))\n",
    "\n",
    "    \n",
    "# Display the best theta\n",
    "print(\"\\n\\nBest Score: {}\".format(best_score))\n",
    "print(\"Best theta:\\n{}\\n\".format(best_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a logistic regression function to set up graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_graph(X, y):\n",
    "    theta = tf.Variable(initial_value=np.random.random((2, 1))*2-1, \n",
    "                        dtype=tf.float32, name=\"weights\")\n",
    "\n",
    "    # Loss function\n",
    "    logits = tf.matmul(X, theta, name=\"logits\")\n",
    "    p = tf.sigmoid(logits)\n",
    "    loss_func = tf.losses.log_loss(y, p)\n",
    "    loss_summ = tf.summary.scalar('log_loss', loss_func)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate)\n",
    "    training_op = optimizer.minimize(loss_func)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    return p, theta, loss_func, loss_summ, init, saver, training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "p, theta, loss_func, loss_summ, init, saver, training_op = log_reg_graph(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.794572\n",
      "Epoch: 500 \tLoss: 0.474075\n",
      "Epoch: 1000 \tLoss: 0.400489\n",
      "Epoch: 1500 \tLoss: 0.36673\n",
      "Epoch: 2000 \tLoss: 0.34708\n",
      "Epoch: 2500 \tLoss: 0.334316\n",
      "Epoch: 3000 \tLoss: 0.325553\n",
      "Epoch: 3500 \tLoss: 0.319131\n",
      "Epoch: 4000 \tLoss: 0.314524\n",
      "Epoch: 4500 \tLoss: 0.311064\n",
      "Epoch: 5000 \tLoss: 0.308395\n",
      "Epoch: 5500 \tLoss: 0.306363\n",
      "Epoch: 6000 \tLoss: 0.304623\n",
      "Epoch: 6500 \tLoss: 0.303336\n",
      "Epoch: 7000 \tLoss: 0.302274\n",
      "Epoch: 7500 \tLoss: 0.301429\n",
      "Epoch: 8000 \tLoss: 0.300714\n",
      "Epoch: 8500 \tLoss: 0.300119\n",
      "Epoch: 9000 \tLoss: 0.29961\n",
      "Epoch: 9500 \tLoss: 0.299224\n",
      "Epoch: 10000 \tLoss: 0.298844\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./tmp/logreg_checkpoint.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./models/my_logreg_model\"\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = get_moons_batch()\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss_func, loss_summ],\n",
    "                                         feed_dict={X: X_moons, y: y_moons.reshape(-1, 1)})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = p.eval(feed_dict={X: X_moons, y: y_moons.reshape(-1, 1)})\n",
    "    theta_val = theta.eval()\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.37735367],\n",
       "       [-3.63062119]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_moons.reshape(-1, 1), np.round(y_proba_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to sklearn logistic regression to see if it matches (it should)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1391435 , -2.80044793]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14551799,  0.85448201]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict_proba([[-1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85999999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_moons, y_moons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
