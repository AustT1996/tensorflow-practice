{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train several different types of neural nets to solve MNIST problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./tmp/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(5000, 784)\n",
      "(5000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Verify parts of MNIST data\n",
    "for s in 'train validation test'.split():\n",
    "    tmp = getattr(mnist, s)\n",
    "    print(tmp.images.shape)\n",
    "    print(tmp.labels.shape)\n",
    "    \n",
    "img_size = mnist.train.images.shape[1]\n",
    "n_labels = mnist.train.labels.shape[1]\n",
    "n_train = mnist.train.images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random batch\n",
    "def random_batch(batch_size):\n",
    "    indices = np.random.randint(n_train, size=batch_size)\n",
    "    X_ = mnist.train.images[indices, :]\n",
    "    y_ = mnist.train.labels[indices]\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 784), (5, 10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in random_batch(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Net\n",
    "Make a dense neural net with several layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches per epoch: 429\n",
      "Epoch: 0 \tLoss: 0.371223 \tAccuracy: 0.8956\n",
      "Epoch: 1 \tLoss: 0.305884 \tAccuracy: 0.9154\n",
      "Epoch: 2 \tLoss: 0.274899 \tAccuracy: 0.9232\n",
      "Epoch: 3 \tLoss: 0.253709 \tAccuracy: 0.9276\n",
      "Epoch: 4 \tLoss: 0.237029 \tAccuracy: 0.9334\n",
      "Epoch: 5 \tLoss: 0.223717 \tAccuracy: 0.9336\n",
      "Epoch: 6 \tLoss: 0.202572 \tAccuracy: 0.9432\n",
      "Epoch: 7 \tLoss: 0.194844 \tAccuracy: 0.9442\n",
      "Epoch: 8 \tLoss: 0.186339 \tAccuracy: 0.948\n",
      "Epoch: 9 \tLoss: 0.176538 \tAccuracy: 0.9512\n",
      "Epoch: 10 \tLoss: 0.173116 \tAccuracy: 0.9498\n",
      "Epoch: 11 \tLoss: 0.161841 \tAccuracy: 0.9536\n",
      "Epoch: 12 \tLoss: 0.160125 \tAccuracy: 0.9544\n",
      "Epoch: 13 \tLoss: 0.153496 \tAccuracy: 0.9562\n",
      "Epoch: 14 \tLoss: 0.145711 \tAccuracy: 0.9574\n",
      "Epoch: 15 \tLoss: 0.145038 \tAccuracy: 0.957\n",
      "Epoch: 16 \tLoss: 0.135227 \tAccuracy: 0.9608\n",
      "Epoch: 17 \tLoss: 0.137797 \tAccuracy: 0.9602\n",
      "Epoch: 18 \tLoss: 0.129534 \tAccuracy: 0.9616\n",
      "Epoch: 19 \tLoss: 0.125262 \tAccuracy: 0.9618\n",
      "Epoch: 20 \tLoss: 0.127384 \tAccuracy: 0.9616\n",
      "Epoch: 21 \tLoss: 0.125475 \tAccuracy: 0.9636\n",
      "Epoch: 22 \tLoss: 0.121322 \tAccuracy: 0.965\n",
      "Epoch: 23 \tLoss: 0.116162 \tAccuracy: 0.9664\n",
      "Epoch: 24 \tLoss: 0.115368 \tAccuracy: 0.9674\n",
      "Epoch: 25 \tLoss: 0.115428 \tAccuracy: 0.9644\n",
      "Epoch: 26 \tLoss: 0.113652 \tAccuracy: 0.966\n",
      "Epoch: 27 \tLoss: 0.113023 \tAccuracy: 0.9656\n",
      "Epoch: 28 \tLoss: 0.10776 \tAccuracy: 0.9698\n",
      "Epoch: 29 \tLoss: 0.108745 \tAccuracy: 0.9674\n",
      "Epoch: 30 \tLoss: 0.106564 \tAccuracy: 0.9672\n",
      "Epoch: 31 \tLoss: 0.104282 \tAccuracy: 0.9682\n",
      "Epoch: 32 \tLoss: 0.101868 \tAccuracy: 0.9726\n",
      "Epoch: 33 \tLoss: 0.102863 \tAccuracy: 0.968\n",
      "Epoch: 34 \tLoss: 0.101004 \tAccuracy: 0.9686\n",
      "Epoch: 35 \tLoss: 0.0983649 \tAccuracy: 0.9692\n",
      "Epoch: 36 \tLoss: 0.0971508 \tAccuracy: 0.9704\n",
      "Epoch: 37 \tLoss: 0.0935087 \tAccuracy: 0.9702\n",
      "Epoch: 38 \tLoss: 0.096912 \tAccuracy: 0.9708\n",
      "Epoch: 39 \tLoss: 0.0954039 \tAccuracy: 0.9702\n",
      "Epoch: 40 \tLoss: 0.0941073 \tAccuracy: 0.9732\n",
      "Epoch: 41 \tLoss: 0.0926455 \tAccuracy: 0.971\n",
      "Epoch: 42 \tLoss: 0.0932343 \tAccuracy: 0.9696\n",
      "Epoch: 43 \tLoss: 0.0984613 \tAccuracy: 0.9698\n",
      "Epoch: 44 \tLoss: 0.0982101 \tAccuracy: 0.9698\n",
      "Epoch: 45 \tLoss: 0.0926341 \tAccuracy: 0.9716\n",
      "Epoch: 46 \tLoss: 0.0851942 \tAccuracy: 0.975\n",
      "Epoch: 47 \tLoss: 0.0842897 \tAccuracy: 0.9768\n",
      "Epoch: 48 \tLoss: 0.0906213 \tAccuracy: 0.9732\n",
      "Epoch: 49 \tLoss: 0.0889527 \tAccuracy: 0.9738\n",
      "Epoch: 50 \tLoss: 0.0906905 \tAccuracy: 0.9722\n",
      "Epoch: 51 \tLoss: 0.0870423 \tAccuracy: 0.9744\n",
      "Epoch: 52 \tLoss: 0.0919339 \tAccuracy: 0.973\n",
      "Epoch: 53 \tLoss: 0.0887779 \tAccuracy: 0.9708\n",
      "Epoch: 54 \tLoss: 0.0918042 \tAccuracy: 0.972\n",
      "Epoch: 55 \tLoss: 0.0833657 \tAccuracy: 0.975\n",
      "Epoch: 56 \tLoss: 0.0876381 \tAccuracy: 0.975\n",
      "Epoch: 57 \tLoss: 0.0885094 \tAccuracy: 0.9738\n",
      "Epoch: 58 \tLoss: 0.0831775 \tAccuracy: 0.9762\n",
      "Epoch: 59 \tLoss: 0.0887893 \tAccuracy: 0.9722\n",
      "Epoch: 60 \tLoss: 0.088456 \tAccuracy: 0.9746\n",
      "Epoch: 61 \tLoss: 0.0872919 \tAccuracy: 0.9736\n",
      "Epoch: 62 \tLoss: 0.0854645 \tAccuracy: 0.974\n",
      "Epoch: 63 \tLoss: 0.0848582 \tAccuracy: 0.9746\n",
      "Epoch: 64 \tLoss: 0.0867331 \tAccuracy: 0.9738\n",
      "Epoch: 65 \tLoss: 0.0880856 \tAccuracy: 0.975\n",
      "Epoch: 66 \tLoss: 0.0807046 \tAccuracy: 0.9748\n",
      "Epoch: 67 \tLoss: 0.083629 \tAccuracy: 0.9754\n",
      "Epoch: 68 \tLoss: 0.0875869 \tAccuracy: 0.9718\n",
      "Epoch: 69 \tLoss: 0.084737 \tAccuracy: 0.9762\n",
      "Epoch: 70 \tLoss: 0.0828094 \tAccuracy: 0.975\n",
      "Epoch: 71 \tLoss: 0.0826957 \tAccuracy: 0.9746\n",
      "Epoch: 72 \tLoss: 0.0848088 \tAccuracy: 0.9764\n",
      "Epoch: 73 \tLoss: 0.0839734 \tAccuracy: 0.9744\n",
      "Epoch: 74 \tLoss: 0.0797668 \tAccuracy: 0.9754\n",
      "Epoch: 75 \tLoss: 0.0804012 \tAccuracy: 0.9758\n",
      "Epoch: 76 \tLoss: 0.0804869 \tAccuracy: 0.9752\n",
      "Epoch: 77 \tLoss: 0.0761213 \tAccuracy: 0.9796\n",
      "Epoch: 78 \tLoss: 0.0824732 \tAccuracy: 0.9776\n",
      "Epoch: 79 \tLoss: 0.0802691 \tAccuracy: 0.9768\n",
      "Epoch: 80 \tLoss: 0.0850312 \tAccuracy: 0.9738\n",
      "Epoch: 81 \tLoss: 0.0782166 \tAccuracy: 0.9788\n",
      "Epoch: 82 \tLoss: 0.0829021 \tAccuracy: 0.9756\n",
      "Epoch: 83 \tLoss: 0.0853015 \tAccuracy: 0.9754\n",
      "Epoch: 84 \tLoss: 0.0838554 \tAccuracy: 0.9762\n",
      "Epoch: 85 \tLoss: 0.0851535 \tAccuracy: 0.9736\n",
      "Epoch: 86 \tLoss: 0.0861218 \tAccuracy: 0.9742\n",
      "Epoch: 87 \tLoss: 0.0826221 \tAccuracy: 0.9752\n",
      "Epoch: 88 \tLoss: 0.0802572 \tAccuracy: 0.9786\n",
      "Epoch: 89 \tLoss: 0.0796542 \tAccuracy: 0.977\n",
      "Epoch: 90 \tLoss: 0.0851622 \tAccuracy: 0.9744\n",
      "Epoch: 91 \tLoss: 0.0813013 \tAccuracy: 0.9766\n",
      "Epoch: 92 \tLoss: 0.084815 \tAccuracy: 0.9738\n",
      "Epoch: 93 \tLoss: 0.0798867 \tAccuracy: 0.9796\n",
      "Epoch: 94 \tLoss: 0.081748 \tAccuracy: 0.9772\n",
      "Epoch: 95 \tLoss: 0.0812815 \tAccuracy: 0.978\n",
      "Epoch: 96 \tLoss: 0.0830298 \tAccuracy: 0.9758\n",
      "Epoch: 97 \tLoss: 0.081657 \tAccuracy: 0.9772\n",
      "Epoch: 98 \tLoss: 0.0790763 \tAccuracy: 0.9798\n",
      "Epoch: 99 \tLoss: 0.0786721 \tAccuracy: 0.9774\n",
      "Epoch: 100 \tLoss: 0.086327 \tAccuracy: 0.9754\n",
      "Epoch: 101 \tLoss: 0.0770727 \tAccuracy: 0.9782\n",
      "Epoch: 102 \tLoss: 0.0812723 \tAccuracy: 0.977\n",
      "Epoch: 103 \tLoss: 0.0782889 \tAccuracy: 0.9782\n",
      "Epoch: 104 \tLoss: 0.0843341 \tAccuracy: 0.9766\n",
      "Epoch: 105 \tLoss: 0.0813985 \tAccuracy: 0.9782\n",
      "Epoch: 106 \tLoss: 0.0795247 \tAccuracy: 0.9786\n",
      "Epoch: 107 \tLoss: 0.079989 \tAccuracy: 0.9786\n",
      "Epoch: 108 \tLoss: 0.0815968 \tAccuracy: 0.9766\n",
      "Epoch: 109 \tLoss: 0.080755 \tAccuracy: 0.976\n",
      "Epoch: 110 \tLoss: 0.0791503 \tAccuracy: 0.975\n",
      "Epoch: 111 \tLoss: 0.0784024 \tAccuracy: 0.9742\n",
      "Epoch: 112 \tLoss: 0.0828227 \tAccuracy: 0.9772\n",
      "Epoch: 113 \tLoss: 0.0807388 \tAccuracy: 0.977\n",
      "Epoch: 114 \tLoss: 0.0778056 \tAccuracy: 0.978\n",
      "Epoch: 115 \tLoss: 0.080684 \tAccuracy: 0.9772\n",
      "Epoch: 116 \tLoss: 0.0772136 \tAccuracy: 0.9778\n",
      "Epoch: 117 \tLoss: 0.0777222 \tAccuracy: 0.9764\n",
      "Epoch: 118 \tLoss: 0.0761092 \tAccuracy: 0.9784\n",
      "Epoch: 119 \tLoss: 0.080425 \tAccuracy: 0.9762\n",
      "Epoch: 120 \tLoss: 0.0832845 \tAccuracy: 0.9774\n",
      "Epoch: 121 \tLoss: 0.0836893 \tAccuracy: 0.9766\n",
      "Epoch: 122 \tLoss: 0.0831818 \tAccuracy: 0.9774\n",
      "Epoch: 123 \tLoss: 0.0854399 \tAccuracy: 0.9764\n",
      "Epoch: 124 \tLoss: 0.0840001 \tAccuracy: 0.9778\n",
      "Epoch: 125 \tLoss: 0.0848053 \tAccuracy: 0.9776\n",
      "Epoch: 126 \tLoss: 0.0797479 \tAccuracy: 0.9796\n",
      "Epoch: 127 \tLoss: 0.0811086 \tAccuracy: 0.9788\n",
      "Epoch: 128 \tLoss: 0.0820547 \tAccuracy: 0.9788\n",
      "Epoch: 129 \tLoss: 0.0864475 \tAccuracy: 0.9796\n",
      "Epoch: 130 \tLoss: 0.0870686 \tAccuracy: 0.9758\n",
      "Epoch: 131 \tLoss: 0.08218 \tAccuracy: 0.9784\n",
      "Epoch: 132 \tLoss: 0.0815544 \tAccuracy: 0.9786\n",
      "Epoch: 133 \tLoss: 0.0778162 \tAccuracy: 0.978\n",
      "Epoch: 134 \tLoss: 0.0806222 \tAccuracy: 0.9778\n",
      "Epoch: 135 \tLoss: 0.0849455 \tAccuracy: 0.9772\n",
      "Epoch: 136 \tLoss: 0.0833118 \tAccuracy: 0.976\n",
      "Epoch: 137 \tLoss: 0.0835459 \tAccuracy: 0.9758\n",
      "Epoch: 138 \tLoss: 0.0839141 \tAccuracy: 0.977\n",
      "Epoch: 139 \tLoss: 0.082017 \tAccuracy: 0.9766\n",
      "Epoch: 140 \tLoss: 0.084548 \tAccuracy: 0.9764\n",
      "Epoch: 141 \tLoss: 0.0829192 \tAccuracy: 0.9772\n",
      "Epoch: 142 \tLoss: 0.0893387 \tAccuracy: 0.9762\n",
      "Epoch: 143 \tLoss: 0.0776239 \tAccuracy: 0.9782\n",
      "Epoch: 144 \tLoss: 0.0827837 \tAccuracy: 0.9772\n",
      "Epoch: 145 \tLoss: 0.0809807 \tAccuracy: 0.9788\n",
      "Epoch: 146 \tLoss: 0.0835721 \tAccuracy: 0.978\n",
      "Epoch: 147 \tLoss: 0.08601 \tAccuracy: 0.9776\n",
      "Epoch: 148 \tLoss: 0.0824571 \tAccuracy: 0.9788\n",
      "Epoch: 149 \tLoss: 0.0831837 \tAccuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "# Network architecture params\n",
    "n_h1 = 300\n",
    "n_h2 = 150\n",
    "n_h3 = 35\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training params\n",
    "n_epochs = 150\n",
    "batch_size = 128\n",
    "n_batches = n_train // batch_size\n",
    "print(\"Number of Batches per epoch: {}\".format(n_batches))\n",
    "\n",
    "# Saving data\n",
    "name = \"mnist_vanilla_net\"\n",
    "model_dir = os.path.join('.', 'models', name)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = os.path.join('.', 'tf_logs', name+now)\n",
    "for d in [model_dir, log_dir]:\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir, \"model_ckpt.ckpt\")\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = os.path.join(model_dir, \"final_model.nn\")\n",
    "\n",
    "# Make the actual neural net\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # Inputs and outputs\n",
    "    X = tf.placeholder(tf.float32, shape=(None, img_size), name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=(None, n_labels), name=\"y\")\n",
    "\n",
    "    # Hidden layers\n",
    "    h1_pre = tf.layers.dense(X, n_h1, activation=tf.nn.elu, name=\"h1_pre\")\n",
    "    h1 = tf.nn.dropout(h1_pre, keep_prob=0.9, name=\"h1\")\n",
    "    h2_pre = tf.layers.dense(h1, n_h2, activation=tf.nn.elu, name=\"h2_pre\")\n",
    "    h2 = tf.nn.dropout(h2_pre, keep_prob=0.95, name=\"h2\")\n",
    "    h3 = tf.layers.dense(h2, n_h3, activation=tf.nn.elu, name=\"h3\")\n",
    "    \n",
    "    # Output\n",
    "    logits = tf.layers.dense(h3, n_labels, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy =\\\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=2e-3, momentum=0.9,\n",
    "                                          use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# For evaluation\n",
    "with tf.name_scope(\"eval\"):\n",
    "    output_class = tf.argmax(y, 1)\n",
    "    correct = tf.nn.in_top_k(logits, tf.argmax(y, 1), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "#     correct = tf.nn.in_top_k(logits, y, 1)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"admin\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "# Run the training\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)   \n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        \n",
    "        # Run batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # Run validation\n",
    "        val_feed_dict = {X: mnist.validation.images, \n",
    "                         y: mnist.validation.labels}        \n",
    "        loss_val, acc_val, summary_str = sess.run([loss, accuracy, loss_summary], \n",
    "                                         feed_dict=val_feed_dict)\n",
    "\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val, \"\\tAccuracy:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from .\\models\\mnist_vanilla_net\\final_model.nn\n",
      "Accuracy: 0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "with tf.Session() as sess:\n",
    "    # if the checkpoint file exists, restore the model and load the epoch number\n",
    "    print(\"Loading model\")\n",
    "    saver.restore(sess, final_model_path)\n",
    "\n",
    "    acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, \n",
    "        y: mnist.test.labels\n",
    "    })\n",
    "    print(\"Accuracy: {}\".format(acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning using He initialization, training only on digits 0 to 4, followed by transfer learning for the rest of the variables\n",
    "This will be used in transfer learning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 0 to 4 training data\n",
    "def filter_04(X, y):\n",
    "    indices = np.argmax(y, 1) < 5\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def next_batch04(size):\n",
    "    return filter_04(*mnist.train.next_batch(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_dir\n",
    "del log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches per epoch: 429\n",
      "Epoch: 0 \tLoss: 0.0640176 \tAccuracy: 0.980844\n",
      "Epoch: 1 \tLoss: 0.0538823 \tAccuracy: 0.984754\n",
      "Epoch: 2 \tLoss: 0.0376537 \tAccuracy: 0.988272\n",
      "Epoch: 3 \tLoss: 0.0700493 \tAccuracy: 0.97889\n",
      "Epoch: 4 \tLoss: 0.0464074 \tAccuracy: 0.988272\n"
     ]
    }
   ],
   "source": [
    "# Network architecture params\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training params\n",
    "n_epochs = 5\n",
    "batch_size = 128\n",
    "n_batches = n_train // batch_size\n",
    "print(\"Number of Batches per epoch: {}\".format(n_batches))\n",
    "\n",
    "# Saving data\n",
    "name = \"mnist_deep_net1\"\n",
    "model_dir04 = os.path.join('.', 'models', name)\n",
    "log_dir04 = os.path.join('.', 'tf_logs', name)\n",
    "for d in [model_dir04, log_dir04]:\n",
    "    if not os.path.isdir(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir04, \"model_ckpt.ckpt\")\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "checkpoint_path_best = os.path.join(model_dir04, \"best_model_ckpt.ckpt\")\n",
    "final_model_path = os.path.join(model_dir04, \"final_model.nn\")\n",
    "\n",
    "# Make the actual neural net\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # Inputs and outputs\n",
    "    X = tf.placeholder(tf.float32, shape=(None, img_size), name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=(None, n_labels), name=\"y\")\n",
    "\n",
    "    # Hidden layers\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\n",
    "    kwargs = {\"activation\": tf.nn.elu, \"kernel_initializer\": he_init}\n",
    "    h1 = tf.layers.dense(X, 100, name=\"h1\", **kwargs)\n",
    "    h2 = tf.layers.dense(h1, 100, name=\"h2\", **kwargs)\n",
    "    h3 = tf.layers.dense(h2, 100, name=\"h3\", **kwargs)\n",
    "    h4 = tf.layers.dense(h3, 100, name=\"h4\", **kwargs)\n",
    "    h5 = tf.layers.dense(h4, 100, name=\"h5\", **kwargs)\n",
    "    \n",
    "    # Output\n",
    "    logits = tf.layers.dense(h5, n_labels, name=\"logits\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "    xentropy =\\\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# For evaluation\n",
    "with tf.name_scope(\"eval\"):\n",
    "    output_class = tf.argmax(y, 1)\n",
    "    correct = tf.nn.in_top_k(logits, tf.argmax(y, 1), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope(\"admin\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    file_writer = tf.summary.FileWriter(log_dir04, tf.get_default_graph())\n",
    "\n",
    "# Run the training\n",
    "highest_acc = 0.\n",
    "epochs_with_no_improvement = 0\n",
    "max_epochs_with_no_improvement = 10\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)   \n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        \n",
    "        # Run batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = next_batch04(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            \n",
    "            # Run validation\n",
    "            X_val_04, y_val_04 = filter_04(mnist.validation.images, \n",
    "                                           mnist.validation.labels)\n",
    "            val_feed_dict = {X: X_val_04, \n",
    "                             y: y_val_04}        \n",
    "            loss_val, acc_val, summary_str = sess.run([loss, accuracy, loss_summary], \n",
    "                                             feed_dict=val_feed_dict)\n",
    "\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val, \"\\tAccuracy:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "            # Early stopping\n",
    "            if acc_val > highest_acc:\n",
    "                highest_acc = acc_val\n",
    "                epochs_with_no_improvement = 0\n",
    "                saver.save(sess, checkpoint_path_best)\n",
    "            else:\n",
    "                epochs_with_no_improvement += 1\n",
    "            if epochs_with_no_improvement > max_epochs_with_no_improvement:\n",
    "                print(\"Early stopping due to no improvement\")\n",
    "\n",
    "    saver.save(sess, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from .\\models\\mnist_deep_net1\\final_model.nn\n",
      "Accuracy on 0-4: 0.9900758862495422\n",
      "Accuracy on all: 0.5088000297546387\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "with tf.Session() as sess:\n",
    "    # if the checkpoint file exists, restore the model and load the epoch number\n",
    "    print(\"Loading model\")\n",
    "    saver.restore(sess, final_model_path)\n",
    "    X_test_04, y_test_04 = filter_04(mnist.test.images, mnist.test.labels)\n",
    "    acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: X_test_04, \n",
    "        y: y_test_04\n",
    "    })\n",
    "    print(\"Accuracy on 0-4: {}\".format(acc_val))\n",
    "\n",
    "    acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, \n",
    "        y: mnist.test.labels\n",
    "    })\n",
    "    print(\"Accuracy on all: {}\".format(acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning for the rest of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches per epoch: 429\n",
      "INFO:tensorflow:Restoring parameters from ./models/mnist_deep_net1/final_model.nn\n",
      "Epoch: 0 \tLoss: 0.365271 \tAccuracy: 0.88534\n",
      "Epoch: 1 \tLoss: 0.339092 \tAccuracy: 0.889025\n",
      "Epoch: 2 \tLoss: 0.34577 \tAccuracy: 0.886568\n",
      "Accuracy on 0-4: 0.8679283857345581\n",
      "Accuracy on all: 0.4219000041484833\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(r\".\\models\\mnist_deep_net1\\final_model.nn.meta\")\n",
    "\n",
    "# Get the inputs and outputs\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"dnn/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"dnn/y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"loss/Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                     scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(1e-2, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "output_class = tf.argmax(y, 1)\n",
    "correct = tf.nn.in_top_k(logits, tf.argmax(y, 1), 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n",
    "\n",
    "# Getting 5-9 data\n",
    "def filter_59(X, y):\n",
    "    indices = np.argmax(y, 1) >= 5\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def next_batch59(size):\n",
    "    return filter_59(*mnist.train.next_batch(size))\n",
    "\n",
    "# Training params\n",
    "n_epochs = 3\n",
    "batch_size = 128\n",
    "n_batches = n_train // batch_size\n",
    "print(\"Number of Batches per epoch: {}\".format(n_batches))\n",
    "\n",
    "# Run the training\n",
    "highest_acc = 0.\n",
    "epochs_with_no_improvement = 0\n",
    "max_epochs_with_no_improvement = 10\n",
    "with tf.Session() as sess:\n",
    "    restore_saver.restore(sess, './models/mnist_deep_net1/final_model.nn')\n",
    "    start_epoch = 0\n",
    "    sess.run(init)\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        \n",
    "        # Run batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = next_batch59(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            \n",
    "            # Run validation\n",
    "            X_val_59, y_val_59 = filter_59(mnist.validation.images, \n",
    "                                           mnist.validation.labels)\n",
    "            val_feed_dict = {X: X_val_59, \n",
    "                             y: y_val_59}        \n",
    "            loss_val, acc_val = sess.run([loss, accuracy], \n",
    "                                             feed_dict=val_feed_dict)\n",
    "\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val, \"\\tAccuracy:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "            # Early stopping\n",
    "            if acc_val > highest_acc:\n",
    "                highest_acc = acc_val\n",
    "                epochs_with_no_improvement = 0\n",
    "                saver.save(sess, checkpoint_path_best)\n",
    "            else:\n",
    "                epochs_with_no_improvement += 1\n",
    "            if epochs_with_no_improvement > max_epochs_with_no_improvement:\n",
    "                print(\"Early stopping due to no improvement\")\n",
    "                \n",
    "    \n",
    "    # Testing\n",
    "    X_test_59, y_test_59 = filter_59(mnist.test.images, mnist.test.labels)\n",
    "    acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: X_test_59, \n",
    "        y: y_test_59\n",
    "    })\n",
    "    print(\"Accuracy on 0-4: {}\".format(acc_val))\n",
    "\n",
    "    acc_val = sess.run(accuracy, feed_dict={\n",
    "        X: mnist.test.images, \n",
    "        y: mnist.test.labels\n",
    "    })\n",
    "    print(\"Accuracy on all: {}\".format(acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understandably, just training the last layer on 5-9 destroys its accuracy on 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
